{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "001d4d22-00dd-4b65-8a23-b3eef0241ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afa5f341-0f58-4c64-a5bc-328dd0b35864",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG19(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(VGG19, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1), \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4f58c6a-574a-42ce-afcc-a436c5fb3ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b411a78b-2997-4a12-af59-e7bc0dc52d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a8678c7-68db-43d5-9724-98013fe2c626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67f2ee6c-1629-482c-a62c-08847b44b6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_processing(image_path,size=(224,224)):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = image.resize(size)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]) \n",
    "    tensor = transform(image)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    tensor = tensor.float()\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15d1d0c3-4105-485c-8be6-ad253d1a46a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self,csv_file):\n",
    "        self.data = pd.read_csv(csv_file).iloc[:5000]\n",
    "        self.additional_data = []\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = f\"Downloads/cifar100-image-classification/{self.data.iloc[idx,0]}\"\n",
    "        label = self.data.iloc[idx, 1]\n",
    "        image = image_processing(img_path)\n",
    "        return image,label\n",
    "    def append_data(self, image_tensor, label):\n",
    "        self.additional_data.append((image_tensor, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c598fbc-c538-4461-b983-fcfb25a4eba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"Downloads/cifar100dataset.csv\"\n",
    "dataset = ImageDataset(csv_file)\n",
    "dataloader = DataLoader(dataset,batch_size=25,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc1c28c4-bb9d-45c2-88e3-b27e794417eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(dataset.data['cattle'].unique()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6312dc6d-c301-4ffd-b0c9-4d4854ab07b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbf4dc11-20e7-4c9a-93a4-96e40fa7eaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Custommodel = VGG19(num_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd211041-ef75-4ae4-99bb-2f40fe2e9615",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = dataset.data['cattle'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86797017-71ac-42b4-ac6e-45c47b9e000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_idx = {classes[i]: i for i in range(len(classes))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fabffd84-a24c-48d9-8a7e-bc3abd1e71e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_class = {i:classes[i] for i in range(len(classes))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55d9274-3859-4242-a670-d77bca4cbb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(Custommodel.parameters(),lr=0.005)\n",
    "\n",
    "for epoch in range(20):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs,label in dataset:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = Custommodel(inputs)\n",
    "        labelidx = class_to_idx[label]\n",
    "        labelidx = torch.tensor([labelidx])\n",
    "        labelidx = labelidx.long()\n",
    "        loss = criterion(outputs,labelidx)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        k,predicted = torch.max(outputs, 1)\n",
    "        total += 1\n",
    "        correct += (predicted.item() == labelidx.item())\n",
    "    print(f\"Epoch {epoch + 1}/{20}, Loss: {running_loss/len(dataset)}, Accuracy: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76035441-983c-410b-8111-31854d5fb8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vgg19(nn.Module):\n",
    "    def __init__(self,model):\n",
    "        super(VGG19ForStyleTransfer,self).__init__()\n",
    "        self.model = model\n",
    "        self.required_layers = [0,5,10,19,28] \n",
    "    def extract_features(self,x):\n",
    "        features = []\n",
    "        for layer_num,layer in enumerate(self.model.features):\n",
    "            x = layer(x)\n",
    "            if layer_num in self.required_layers:\n",
    "                features.append(x)\n",
    "        return features\n",
    "    def forward(self, x):\n",
    "        features = self.extract_features(x)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0d2faa-261d-43b9-94c9-290306380567",
   "metadata": {},
   "outputs": [],
   "source": [
    "styletransfer = Vgg19(Custommodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe0f707-8795-4c21-9ec7-f15c0fa64873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(content_feature,noise_feature):\n",
    "    loss = 0\n",
    "    for content_f,noise_f in zip(content_features,noise_features):\n",
    "        loss+=(torch.mean((content_f-noise_f)**2))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c928234-881c-448f-ac4d-2685f494e5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(features):\n",
    "    b,c,h,w = features.size()\n",
    "    features = features.view(c,h*w)\n",
    "    grammatrix = torch.mm(features,features.t())\n",
    "    return grammatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85ab03f-dfbd-4208-bc33-1661cf2b9319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_loss(style_feature,noise_feature):\n",
    "    loss = 0\n",
    "    for style_f,noise_f in zip(style_feature,noise_feature):\n",
    "        gram_style = gram_matrix(style_f)\n",
    "        gram_noise = gram_matrix(noise_f)\n",
    "        N = gram_style.size(0)\n",
    "        M = gram_noise.size(1)\n",
    "        loss+=(torch.mean((gram_style-gram_noise)**2))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8352eda8-5acc-4625-aac7-48a35474b94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_w = 8\n",
    "style_w = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ad2b9d-ec83-4b88-8ab7-2048093b93ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_loss(content_w,style_w,content_l,style_l):\n",
    "    loss = (content_w*content_l)+(style_w*style_l)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484f107f-56d4-4151-abdb-7697cd1c606b",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image_path = 'Downloads/Tesla_circa_1890.jpeg'\n",
    "style_path = 'Downloads/style_image.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be698ee-d10f-42a3-b61b-4d2a3493a35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_image = image_processing(style_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cbe937-da1f-4c76-ad98-ac052b79fb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a73ddae-c3c2-4a49-8f70-327e205016d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'Downloads/output_style_images'\n",
    "os.makedirs(output_path,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8bbbd2-aa32-4730-9563-4af25ac2bdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = image_processing(content_image_path)\n",
    "noise_image = content_image.clone().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a16189-512b-424d-a3f2-6464cd2ef66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer2=optim.Adam([noise_image],lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6196270d-5b69-4d86-a5db-367c724969aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deprocess_image(image_tensor):\n",
    "    image_tensor = image_tensor.clone().squeeze(0)\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    image_tensor = image_tensor * std + mean\n",
    "    image_tensor = image_tensor.clamp(0, 1)\n",
    "    image_np = image_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "    return cv2.cvtColor((image_np * 255).astype('uint8'), cv2.COLOR_RGB2BGR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cfb9da-ac66-4673-888f-438c183c597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3000):\n",
    "    optimizer2.zero_grad()\n",
    "    content_features = styletransfer(content_image)\n",
    "    style_features = styletransfer(style_image)\n",
    "    noise_features = styletransfer(noise_image)\n",
    "    content_l = content_loss(content_features,noise_features)\n",
    "    style_l = style_loss(style_features, noise_features)\n",
    "    t_loss = total_loss(content_w, style_w, content_l, style_l)\n",
    "    t_loss.backward()\n",
    "    optimizer2.step()\n",
    "output_image = deprocess_image(noise_image)\n",
    "output_image_name = \"Output_stylized_img.jpg\"\n",
    "cv2.imwrite(os.path.join(output_path,output_image_name),output_image)\n",
    "print(\"Style transfer complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
